{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWidqa6S2acV",
        "outputId": "26cf2a48-f423-4815-dc21-2448058c0c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Collecting tomlkit (from pennylane)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.40 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.40->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Downloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, tomlkit, scipy-openblas32, rustworkx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, autoray, nvidia-cusparse-cu12, nvidia-cudnn-cu12, diastatic-malt, nvidia-cusolver-cu12, pennylane-lightning, pennylane\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.1 diastatic-malt-2.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane torch torchvision numpy matplotlib scikit-learn\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LbqAgL8pPYV",
        "outputId": "cdafd404-eeb4-4289-dc8b-713c7e982518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 14.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 508kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.34MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.97MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size: 48000\n",
            "Validation Data Size: 12000\n",
            "Test Data Size: 10000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "# Define transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load MNIST Dataset\n",
        "train_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_data = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# # Randomly select 10,000 images from the training dataset\n",
        "# subset_size = 10000\n",
        "# random_indices = torch.randperm(len(train_data)).tolist()[:subset_size]  # Randomly shuffle and select 10,000 samples\n",
        "\n",
        "# # Create a random subset of the training data\n",
        "# train_data_subset = Subset(train_data, random_indices)\n",
        "\n",
        "# # Split the subset into train and validation sets (80% train, 20% validation)\n",
        "# train_size = int(0.8 * len(train_data_subset))\n",
        "# val_size = len(train_data_subset) - train_size\n",
        "# train_subset, val_subset = torch.utils.data.random_split(train_data_subset, [train_size, val_size])\n",
        "\n",
        "# # Create DataLoaders for train, validation, and test\n",
        "# batch_size = 128\n",
        "# train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Show data sizes\n",
        "# print(f\"Training Data Size: {len(train_subset)}\")\n",
        "# print(f\"Validation Data Size: {len(val_subset)}\")\n",
        "# print(f\"Test Data Size: {len(test_data)}\")\n",
        "\n",
        "# Split train data into train and validation sets (80% train, 20% validation)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Show data sizes\n",
        "print(f\"Training Data Size: {len(train_data)}\")\n",
        "print(f\"Validation Data Size: {len(val_data)}\")\n",
        "print(f\"Test Data Size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwJknSB1JWfG"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "num_qubits = 8  # Quantum circuit size\n",
        "\n",
        "dev = qml.device(\"default.qubit\", wires=num_qubits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjVzeYASJvpB"
      },
      "outputs": [],
      "source": [
        "# Quantum Circuit\n",
        "def quantum_layer(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(num_qubits)) #q encoding\n",
        "    qml.BasicEntanglerLayers(weights, wires=range(num_qubits)) #q gates\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)] #measurement\n",
        "\n",
        "weight_shapes = {\"weights\": (1, num_qubits, num_qubits)}\n",
        "qnode = qml.QNode(quantum_layer, dev, interface=\"torch\")\n",
        "quantum_net = qml.qnn.TorchLayer(qnode, weight_shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA3DX0wRKBKm"
      },
      "outputs": [],
      "source": [
        "# CNN Feature Extractor\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc = nn.Linear(7 * 7 * 32, num_qubits) #first fc layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x) #CNN layers + Pooling\n",
        "        x = x.view(x.size(0), -1) # Flatten the output\n",
        "        x = self.fc(x) #first fully conn layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrUhsq4KLo5"
      },
      "outputs": [],
      "source": [
        "class HybridQuantumCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = CNNFeatureExtractor()\n",
        "        self.q_layer = quantum_net\n",
        "        self.fc2 = nn.Linear(num_qubits, 10)  # Second Fully Connected Layer (10 classes for MNIST)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # Extract features from CNN\n",
        "        cnn_features = self.cnn(x)  # CNN Layers + Pooling + First Fully Connected Layer\n",
        "\n",
        "        # Pass through quantum layer\n",
        "        quantum_features = self.q_layer(cnn_features)  # Quantum Encoding + Quantum Gates + Measurement\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc2(quantum_features)  # Second Fully Connected Layer\n",
        "\n",
        "        if return_features:\n",
        "            return {\n",
        "                \"cnn_features\": cnn_features,  # Features after CNN + First Fully Connected Layer\n",
        "                \"quantum_features\": quantum_features,  # Features after Quantum Layer\n",
        "                \"output\": output  # Final classification output\n",
        "            }\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Loss Function & Optimizer and Train*"
      ],
      "metadata": {
        "id": "EHY0Ep1qLV_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model\n",
        "model = HybridQuantumCNN()\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Optimizer\n",
        "\n",
        "# Training Function\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print loss for each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZI0PHrXLd3B",
        "outputId": "8a3db59a-9d75-40d2-ecd8-84346e8793bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Loss: 1.8342\n",
            "Epoch [2/15], Loss: 0.8768\n",
            "Epoch [3/15], Loss: 0.1849\n",
            "Epoch [4/15], Loss: 0.0873\n",
            "Epoch [5/15], Loss: 0.0668\n",
            "Epoch [6/15], Loss: 0.0545\n",
            "Epoch [7/15], Loss: 0.0452\n",
            "Epoch [8/15], Loss: 0.0373\n",
            "Epoch [9/15], Loss: 0.0323\n",
            "Epoch [10/15], Loss: 0.0286\n",
            "Epoch [11/15], Loss: 0.0251\n",
            "Epoch [12/15], Loss: 0.0227\n",
            "Epoch [13/15], Loss: 0.0176\n",
            "Epoch [14/15], Loss: 0.0158\n",
            "Epoch [15/15], Loss: 0.0138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"hybrid_model.pth\")"
      ],
      "metadata": {
        "id": "2IUIxCoxOBmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-t3gYDs-49p"
      },
      "source": [
        "# ***Extract Features***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxUWZd-QK4H9"
      },
      "outputs": [],
      "source": [
        "def extract_features(model, dataloader, layer_names):\n",
        "    \"\"\"\n",
        "    Extract features from multiple layers of the model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        dataloader: DataLoader for the dataset.\n",
        "        layer_names: List of names of the layers to extract features from.\n",
        "\n",
        "    Returns:\n",
        "        features_dict: Dictionary containing extracted features from each layer.\n",
        "        labels: Corresponding labels.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    features_dict = {layer_name: [] for layer_name in layer_names}  # Dictionary to store features\n",
        "    labels = []  # List to store corresponding labels\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for images, label in dataloader:\n",
        "            # Forward pass with feature extraction\n",
        "            outputs = model(images, return_features=True)\n",
        "\n",
        "            # Extract features from each specified layer\n",
        "            for layer_name in layer_names:\n",
        "                layer_features = outputs[layer_name].cpu().numpy()  # Move to CPU and convert to numpy\n",
        "                features_dict[layer_name].append(layer_features)\n",
        "\n",
        "            labels.append(label.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "\n",
        "    # Concatenate all batches for each layer\n",
        "    for layer_name in layer_names:\n",
        "        features_dict[layer_name] = np.concatenate(features_dict[layer_name], axis=0)\n",
        "\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "    return features_dict, labels\n",
        "\n",
        "# # Extract features from the trained model\n",
        "# features_dict, labels = extract_features(model, train_loader, [\"cnn_features\", \"quantum_features\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB3aFjvR_UBZ"
      },
      "source": [
        "# ***Randomly Select Layers and Extract Features***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR2KSarD_SW5",
        "outputId": "bcb1d3a7-0e75-49b3-9a12-da59547799e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features from layer 'cnn_features' shape: (48000, 8)\n",
            "Features from layer 'quantum_features' shape: (48000, 8)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Initialize and forward pass through the model\n",
        "model = HybridQuantumCNN()\n",
        "\n",
        "# List of all layer names in the model\n",
        "all_layer_names = [\"cnn_features\", \"quantum_features\"]  # Add more layer names if needed\n",
        "\n",
        "# Randomly select a subset of layers\n",
        "num_layers_to_select = 2  # Number of layers to randomly select\n",
        "random_layer_names = random.sample(all_layer_names, num_layers_to_select)\n",
        "\n",
        "# Extract features from the randomly selected layers\n",
        "features_dict, labels = extract_features(model, train_loader, random_layer_names)\n",
        "\n",
        "# Print the shapes of the extracted features\n",
        "for layer_name, features in features_dict.items():\n",
        "    print(f\"Features from layer '{layer_name}' shape: {features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-2T41b-Kh8s",
        "outputId": "766d414e-b850-4008-bd39-a1a66b3762a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming Distance: 0.0751\n",
            "Inlier detected.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "from scipy.spatial.distance import hamming\n",
        "\n",
        "# Train One-Class SVM on classical features\n",
        "classical_svm = OneClassSVM(gamma='auto', nu=0.1)  # Adjust hyperparameters as needed\n",
        "classical_svm.fit(features_dict[\"cnn_features\"])\n",
        "\n",
        "# Train One-Class SVM on quantum features\n",
        "quantum_svm = OneClassSVM(gamma='auto', nu=0.1)  # Adjust hyperparameters as needed\n",
        "quantum_svm.fit(features_dict[\"quantum_features\"])\n",
        "\n",
        "# Get predictions from the classical and quantum SVMs\n",
        "classical_predictions = classical_svm.predict(features_dict[\"cnn_features\"])\n",
        "quantum_predictions = quantum_svm.predict(features_dict[\"quantum_features\"])\n",
        "\n",
        "# Compute Hamming distance between the predictions\n",
        "hamming_distance = hamming(classical_predictions, quantum_predictions)\n",
        "print(f\"Hamming Distance: {hamming_distance:.4f}\")\n",
        "\n",
        "# Set condition for inliers and outliers\n",
        "d = 0.1  # Adjust as needed\n",
        "if hamming_distance < d:\n",
        "    print(\"Inlier detected.\")\n",
        "else:\n",
        "    print(\"Outlier detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***OOD Detection on New Data***"
      ],
      "metadata": {
        "id": "j_0_x47YFxAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.spatial.distance import hamming"
      ],
      "metadata": {
        "id": "X9lS0qd5cp0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWEPY1hibwqk",
        "outputId": "dd394be1-4cbb-4f67-8eec-79eacc245ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the dataset path\n",
        "root = \"/content/drive/My Drive/ood\"  # Path to the dataset in Google Drive\n",
        "if not os.path.exists(root):\n",
        "    raise FileNotFoundError(f\"The directory '{root}' does not exist.\")\n",
        "\n",
        "# Verify the subdirectories exist\n",
        "subdirs = [\"cats\", \"dogs\"]\n",
        "for subdir in subdirs:\n",
        "    subdir_path = os.path.join(root, subdir)\n",
        "    if not os.path.exists(subdir_path):\n",
        "        raise FileNotFoundError(f\"The subdirectory '{subdir_path}' does not exist.\")\n",
        "\n",
        "# Define transformations (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Resize images to 28x28 (same as MNIST)\n",
        "    transforms.Grayscale(),  # Convert to grayscale (if needed)\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
        "])\n",
        "\n",
        "# Load out-of-distribution data (e.g., cat, dog images)\n",
        "ood_data = datasets.ImageFolder(root=root, transform=transform)\n",
        "ood_loader = DataLoader(ood_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Extract features from OOD data\n",
        "features_dict_ood, _ = extract_features(model, ood_loader, [\"cnn_features\", \"quantum_features\"])\n",
        "\n",
        "# Predict using One-Class SVM\n",
        "classical_predictions_ood = classical_svm.predict(features_dict_ood[\"cnn_features\"])\n",
        "quantum_predictions_ood = quantum_svm.predict(features_dict_ood[\"quantum_features\"])\n",
        "\n",
        "# Compute Hamming distance for OOD data\n",
        "hamming_distance_ood = hamming(classical_predictions_ood, quantum_predictions_ood)\n",
        "print(f\"Hamming Distance for OOD data: {hamming_distance_ood:.4f}\")\n",
        "\n",
        "# Set condition for OOD data\n",
        "d = 0.1  # Adjust as needed\n",
        "if hamming_distance_ood < d:\n",
        "    print(\"OOD data is classified as inlier.\")\n",
        "else:\n",
        "    print(\"OOD data is classified as outlier.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjawxBq9Fk1i",
        "outputId": "15efa5da-f1d6-4212-94ea-157ef0cedea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming Distance for OOD data: 0.1064\n",
            "OOD data is classified as outlier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Combine predictions and labels\n",
        "all_predictions = np.concatenate([classical_predictions, classical_predictions_ood])\n",
        "all_labels = np.concatenate([np.ones_like(classical_predictions), -1 * np.ones_like(classical_predictions_ood)])\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "precision = precision_score(all_labels, all_predictions, pos_label=1)\n",
        "recall = recall_score(all_labels, all_predictions, pos_label=1)\n",
        "f1 = f1_score(all_labels, all_predictions, pos_label=1)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLP3z_WfgFHL",
        "outputId": "63a65bb2-6790-4290-ce32-3e267f7af207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8997\n",
            "Precision: 0.9996\n",
            "Recall: 0.9000\n",
            "F1-Score: 0.9472\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}